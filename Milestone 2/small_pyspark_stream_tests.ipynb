{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a0c9ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "528e7572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88abcbdf",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b90cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "def get_vals(w: int = 5):\n",
    "    coefs = np.random.randint(1, 1000, (w,w))\n",
    "    p = np.random.randint(3000, 100000, w)\n",
    "    return coefs, p + ~(p % 2).astype(bool)\n",
    "\n",
    "def fit_poly(row, x):\n",
    "    return np.poly1d(row)(x)\n",
    "\n",
    "def cm_multihash(x: int, counts: int, coefs, p, const: int = 2719):\n",
    "    table = np.zeros((p.shape[0], const))\n",
    "    j = ((np.apply_along_axis(fit_poly, 1, coefs, x) % p) % const).astype(int)\n",
    "    table[np.arange(0, p.shape[0]),j] = counts\n",
    "    return sparse.csr_matrix(table)\n",
    "\n",
    "def sparse_sim(sk1: sparse.csr.csr_matrix, sk2: sparse.csr.csr_matrix):\n",
    "    return sk1.multiply(sk2).sum(axis = 1).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "426ebea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs, p = get_vals()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d76d2",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4f3a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "600b7690",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.process_time()\n",
    "collect_hashes = lines.window(60)\\\n",
    "                        .map(lambda line: line.split(','))\\\n",
    "                        .map(lambda line: (line[0], cm_multihash(int(line[1]), 1, coefs, p)))\\\n",
    "                        .reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "070fc473",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared = collect_hashes.map(lambda line: (1, [line[0], line[1]]))\n",
    "combo = prepared.transform(lambda RDD: RDD.join(RDD).filter(lambda row: row[1][0][0] > row[1][1][0])\\\n",
    "                                          .map(lambda row: [row[1][0][1], row[1][1][1]]))\\\n",
    "                .map(lambda line: sparse_sim(line[-2], line[-1]))\\\n",
    "                .filter(lambda line: line[-1] > tau).count()\\\n",
    "                #.map(lambda line: [line, (time.process_time() - start_time)])\n",
    "                \n",
    "combo.pprint(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b565eb9f",
   "metadata": {},
   "source": [
    "WORKING NOW: 10 rows per second "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d24599b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c11233e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ef23b9",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88beb1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\onii-chan\\\\Documents\\\\JupyterNotebooks\\\\Big Data Management\\\\Milestone 2\\\\checkpoint_dit'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()+'\\\\checkpoint_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "592ff79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Spark\\spark-3.2.0-bin-hadoop3.2\\python\\pyspark\\context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "sc = SparkContext() \n",
    "#ss = SparkSession.builder.appName(\"s1\").getOrCreate()\n",
    "#lines = ss.readStream.format(\"socket\").option(\"host\", \"localhost\").option(\"port\",9000).load()\n",
    "ssc = StreamingContext(sc, 2)\n",
    "lines = ssc.socketTextStream(\"localhost\", 9000)\n",
    "\n",
    "#os.mkdir('checkpoint_dir')\n",
    "ssc.checkpoint(os.getcwd()+'\\\\checkpoint_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b94931d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "lambda cannot contain assignment (<ipython-input-10-3c1ff46a5bd9>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-3c1ff46a5bd9>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    collect_hashes = lines.foreachRDD(lambda rdd: accum= accum + rdd.count())\\\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m lambda cannot contain assignment\n"
     ]
    }
   ],
   "source": [
    "def tmp_fun(a,b):\n",
    "    return a+b\n",
    "\n",
    "def inv_fun(a,b):\n",
    "    return a-b\n",
    "\n",
    "accum = sc.accumulator(0)\n",
    "\n",
    "collect_hashes = lines.foreachRDD(lambda rdd: rdd.count())\\\n",
    "                      .map(lambda line: line.split(','))\\\n",
    "                      .map(lambda line: (line[0], cm_multihash(int(line[1]), 1, coefs, p)))\\\n",
    "                      .reduceByKeyAndWindow(tmp_fun, inv_fun, 60, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7e01b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared = collect_hashes.map(lambda line: (1, [line[0], line[1]]))\n",
    "combo = prepared.transform(lambda RDD: RDD.join(RDD).map(lambda row: [row[1][0][0], row[1][1][0],\n",
    "                                                                      row[1][0][1], row[1][1][1]]))\\\n",
    "                .filter(lambda line: line[0] > line[1])\\\n",
    "                .map(lambda line: [line[0], line[1], sparse_sim(line[-2], line[-1])])\\\n",
    "                .count()\n",
    "                #.filter(lambda line: line[-1] > tau)\\\n",
    "                \n",
    "combo.pprint(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f33fc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-03-25 22:49:32\n",
      "-------------------------------------------\n",
      "1891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2f9dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba5707d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cebd89",
   "metadata": {},
   "source": [
    "### The code below is WORKING code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50826724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Spark\\spark-3.2.0-bin-hadoop3.2\\python\\pyspark\\context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "\n",
    "sc = SparkContext() \n",
    "ssc = StreamingContext(sc, 2)\n",
    "lines = ssc.socketTextStream(\"localhost\", 9000)\n",
    "\n",
    "#os.mkdir('checkpoint_dir')\n",
    "ssc.checkpoint(os.getcwd()+'\\\\checkpoint_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c25df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e4c0f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSparkSessionInstance(sparkConf):\n",
    "    if (\"sparkSessionSingletonInstance\" not in globals()):\n",
    "        globals()[\"sparkSessionSingletonInstance\"] = SparkSession \\\n",
    "            .builder\\\n",
    "            .getOrCreate()\n",
    "    return globals()[\"sparkSessionSingletonInstance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3350493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUpdatesCounter(sparkContext):\n",
    "    if (\"counter\" not in globals()):\n",
    "        globals()[\"counter\"] = sparkContext.accumulator(0)\n",
    "    return globals()[\"counter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f4f6fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmp_fun(a,b):\n",
    "    return a+b\n",
    "\n",
    "def inv_fun(a,b):\n",
    "    return a-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ca45ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def counting(time, rdd):\n",
    "    counter = getUpdatesCounter(rdd.context)\n",
    "    counter.add(rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9fd1ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(time, rdd):\n",
    "    #print(\"========= %s =========\" % str(time))\n",
    "    counter = getUpdatesCounter(rdd.context)\n",
    "    #counter.add(rdd.count())\n",
    "    collect = rdd.map(lambda line: line.split(','))\\\n",
    "                 .map(lambda line: (line[0], cm_multihash(int(line[1]), 1, coefs, p)))\\\n",
    "                 .reduceByKey(tmp_fun)\\\n",
    "                 .map(lambda line: (1, [line[0], line[1]]))\n",
    "    fin = collect.join(collect).map(lambda row: [row[1][0][0], row[1][1][0], row[1][0][1], row[1][1][1]])\\\n",
    "                 .filter(lambda line: line[0] > line[1])\\\n",
    "                 .map(lambda line: [line[0], line[1], sparse_sim(line[-2], line[-1])])\\\n",
    "                 .filter(lambda line: line[2] > tau).count()\n",
    "    print(f'>>>> {str(time)[-8:]}  =====  {fin}  =====  {counter.value}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d061eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.foreachRDD(counting)\n",
    "lines.window(60).foreachRDD(process_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4038e51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> 22:03:58  =====  0  =====  100\n",
      ">>>> 22:04:00  =====  0  =====  300\n",
      ">>>> 22:04:02  =====  0  =====  400\n",
      ">>>> 22:04:04  =====  0  =====  600\n",
      ">>>> 22:04:06  =====  0  =====  800\n",
      ">>>> 22:04:08  =====  0  =====  1000\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8e8754",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef325c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
